{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file extracts the meta data on author rates and switching from the Arxiv dataset on Kaggle. \n",
    "\n",
    "NOTE: you also have to get the ArXiv data from online. I did this here:\n",
    "https://www.kaggle.com/datasets/Cornell-University/arxiv, and I placed the unzipped file\n",
    "\"arxiv-metadata-oai-snapshot.json\" (around 4 GB) inside the folder \"arxiv_data\".\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import plotly.graph_objs as go\n",
    "import kaleido\n",
    "\n",
    "from pathlib import Path\n",
    "root = Path(\".\")\n",
    "data_dir = root / \"arxiv_data\"  # you will have to create this\n",
    "\n",
    "import plotly.express as px\n",
    "import re\n",
    "year_pattern = r'([1-2][0-9]{3})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Some basic functions\n",
    "from basic_utils import *\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data_dir / 'arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data by publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "100000 done\n",
      "200000 done\n",
      "300000 done\n",
      "400000 done\n",
      "500000 done\n",
      "600000 done\n",
      "700000 done\n",
      "800000 done\n",
      "900000 done\n",
      "1000000 done\n",
      "1100000 done\n",
      "1200000 done\n",
      "1300000 done\n",
      "1400000 done\n",
      "1500000 done\n",
      "1600000 done\n",
      "1700000 done\n",
      "1800000 done\n",
      "1900000 done\n",
      "2000000 done\n",
      "2100000 done\n",
      "2200000 done\n",
      "2208915\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas dataframe with each publication information \n",
    "\n",
    "# Create an empty list to accumulate rows\n",
    "rows = []\n",
    "\n",
    "length = 0\n",
    "metadata = get_metadata()\n",
    "for paper in metadata:\n",
    "    if length > MAX_EXTRACTION_LENGTH:\n",
    "        break\n",
    "    paper_data = json.loads(paper)\n",
    "    \n",
    "    # Extract and process the data\n",
    "    categories = list(set([check_in_major_categories(cat) for cat in paper_data['categories'].split() \n",
    "                  if check_in_major_categories(cat) is not None])) # removes duplicates\n",
    "    if len(categories) == 0:\n",
    "        continue  # skip this one, it's a weird paper with no normal tags \n",
    "    paper_id = paper_data['id']\n",
    "    pub_date = extract_decimal_year_of_pub(paper_data['versions'])\n",
    "    authors_parsed = paper_data['authors_parsed']\n",
    "    \n",
    "    # Create a dictionary with the extracted data\n",
    "    row = {'id': paper_id, 'categories': categories, 'pub_date': pub_date, 'authors_parsed': authors_parsed}\n",
    "\n",
    "    # Append the row to the rows list\n",
    "    rows.append(row)\n",
    "    \n",
    "    if length % 100_000 == 0:\n",
    "        print(length, 'done')\n",
    "    length += 1\n",
    "\n",
    "# Create a DataFrame using the accumulated rows\n",
    "columns = ['id', 'categories', 'pub_date', 'authors_parsed']\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique categories and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astro-ph', 'astro-ph.CO', 'astro-ph.EP', 'astro-ph.GA', 'astro-ph.HE', 'astro-ph.IM', 'astro-ph.SR', 'cond-mat.dis-nn', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci', 'cond-mat.other', 'cond-mat.quant-gas', 'cond-mat.soft', 'cond-mat.stat-mech', 'cond-mat.str-el', 'cond-mat.supr-con', 'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG', 'cs.CL', 'cs.CR', 'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC', 'cs.DL', 'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL', 'cs.GL', 'cs.GR', 'cs.GT', 'cs.HC', 'cs.IR', 'cs.IT', 'cs.LG', 'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS', 'cs.NA', 'cs.NE', 'cs.NI', 'cs.OH', 'cs.OS', 'cs.PF', 'cs.PL', 'cs.RO', 'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY', 'econ.EM', 'eess.AS', 'eess.IV', 'eess.SP', 'gr-qc', 'hep-ex', 'hep-lat', 'hep-ph', 'hep-th', 'math-ph', 'math.AC', 'math.AG', 'math.AP', 'math.AT', 'math.CA', 'math.CO', 'math.CT', 'math.CV', 'math.DG', 'math.DS', 'math.FA', 'math.GM', 'math.GN', 'math.GR', 'math.GT', 'math.HO', 'math.IT', 'math.KT', 'math.LO', 'math.MG', 'math.MP', 'math.NA', 'math.NT', 'math.OA', 'math.OC', 'math.PR', 'math.QA', 'math.RA', 'math.RT', 'math.SG', 'math.SP', 'math.ST', 'nlin.AO', 'nlin.CD', 'nlin.CG', 'nlin.PS', 'nlin.SI', 'nucl-ex', 'nucl-th', 'physics.acc-ph', 'physics.ao-ph', 'physics.app-ph', 'physics.atm-clus', 'physics.atom-ph', 'physics.bio-ph', 'physics.chem-ph', 'physics.class-ph', 'physics.comp-ph', 'physics.data-an', 'physics.ed-ph', 'physics.flu-dyn', 'physics.gen-ph', 'physics.geo-ph', 'physics.hist-ph', 'physics.ins-det', 'physics.med-ph', 'physics.optics', 'physics.plasm-ph', 'physics.pop-ph', 'physics.soc-ph', 'physics.space-ph', 'q-bio.BM', 'q-bio.CB', 'q-bio.GN', 'q-bio.MN', 'q-bio.NC', 'q-bio.OT', 'q-bio.PE', 'q-bio.QM', 'q-bio.SC', 'q-bio.TO', 'q-fin.CP', 'q-fin.EC', 'q-fin.GN', 'q-fin.MF', 'q-fin.PM', 'q-fin.PR', 'q-fin.RM', 'q-fin.ST', 'q-fin.TR', 'quant-ph', 'stat.AP', 'stat.CO', 'stat.ME', 'stat.ML', 'stat.OT', 'stat.TH']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a list of all unique categories\n",
    "unique_categories = sorted(set(cat for categories in df['categories'] for cat in categories))\n",
    "\n",
    "def index_to_category(ind):\n",
    "    return unique_categories[ind]\n",
    "\n",
    "def category_to_index(cat):\n",
    "    return unique_categories.index(cat)\n",
    "\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762689\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with the contributions of each unique author name. \n",
    "\n",
    "# Initialize an empty dictionary to store the authors and their papers\n",
    "author_papers = {}\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    paper_id = row['id']\n",
    "    categories = row['categories']\n",
    "    pub_date = row['pub_date']\n",
    "    authors_parsed = row['authors_parsed']\n",
    "\n",
    "    # Iterate through the authors of each paper\n",
    "    for author in authors_parsed:\n",
    "        author_key = \"\".join(author)  # Create a unique author key\n",
    "\n",
    "        # If the author is not in the dictionary, add them with their first paper\n",
    "        if author_key not in author_papers:\n",
    "            author_papers[author_key] = []\n",
    "\n",
    "        # Add the paper information to the author's list of papers\n",
    "        author_papers[author_key].append([paper_id, pub_date, categories])\n",
    "\n",
    "print(len(author_papers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract author status data by date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Initialize the author_status_metadata \n",
    "# Contains a list of categories currently being published by each author during each timebin\n",
    "# as well as the previous author's time bin\n",
    "\n",
    "# An author is only considered active in years where they publish something. \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "author_status_metadata = defaultdict(list)\n",
    "\n",
    "for author_key, papers in author_papers.items():\n",
    "    if not (MIN_PUBLICATIONS <= len(papers) <= MAX_PUBLICATIONS):\n",
    "        continue  # only use authors with a few publications, or not too many (otherwise likely a non-unique name)\n",
    "\n",
    "    bin_size = YEARS_BINSIZE\n",
    "    binned_papers = defaultdict(set)\n",
    "\n",
    "    # Bin the papers by date\n",
    "    for p in papers:\n",
    "        _, date, categories_as_words = p\n",
    "        binned_date = bin_date(date, bin_size)\n",
    "        categories_as_indices = set([category_to_index(item) for item in categories_as_words])\n",
    "        binned_papers[binned_date].update(categories_as_indices)\n",
    "\n",
    "    # Find the min and max binned dates\n",
    "    min_binned_date = bin_date(min(date for date in binned_papers.keys()), bin_size)\n",
    "    max_binned_date = bin_date(max(date for date in binned_papers.keys()), bin_size)\n",
    "\n",
    "    # Iterate over the binned dates in the known range\n",
    "    current_date = min_binned_date\n",
    "    prev_year_categories = set(binned_papers[min_binned_date])\n",
    "    while current_date <= max_binned_date + 0.5 * bin_size:  # add 0.5 to not miss last bin\n",
    "        if binned_papers[current_date]:\n",
    "            current_year_categories = set(binned_papers[current_date])\n",
    "            author_status_metadata[current_date].append([list(current_year_categories), list(prev_year_categories)])\n",
    "        \n",
    "        prev_year_categories = current_year_categories\n",
    "        current_date = bin_date(current_date + 1.01*bin_size, bin_size)  # add 1.01 to make sure to reach next bin\n",
    "        \n",
    "sorted_dates = list(sorted(author_status_metadata.keys()))\n",
    "print(len(sorted_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Now for each bin, we can take the list of papers, and measure things. \n",
    "\n",
    "def count_authors_by_category(author_status_data, unique_categories):\n",
    "    category_count = np.zeros(len(unique_categories))\n",
    "\n",
    "    for author in author_status_data:\n",
    "        current_categories = author[0]\n",
    "        weight = 1 / len(current_categories)\n",
    "        for category in current_categories:\n",
    "            category_count[category] += weight\n",
    "\n",
    "    return category_count\n",
    "\n",
    "def count_transitions(author_status_data, unique_categories):    \n",
    "    \"\"\"\n",
    "    Gets the transition matrix. first index is source, second index is target\n",
    "    The entry is the number of transitions from one field to another. \n",
    "    \"\"\"\n",
    "    \n",
    "    transition_count = np.zeros((len(unique_categories), len(unique_categories)))\n",
    "    \n",
    "    for author in author_status_data:\n",
    "        current_categories = author[0]\n",
    "        previous_categories = author[1]\n",
    "\n",
    "        transition_vector = np.zeros(len(unique_categories))\n",
    "\n",
    "        for cat in current_categories:\n",
    "            transition_vector[cat] += 1 / len(current_categories)        \n",
    "        for cat in previous_categories:\n",
    "            transition_vector[cat] -= 1 / len(previous_categories)\n",
    "\n",
    "        neg_source_mask = transition_vector[previous_categories] < 0\n",
    "        pos_target_mask = transition_vector[current_categories] > 0\n",
    "\n",
    "        for source_index, source_neg in zip(previous_categories, neg_source_mask):\n",
    "            if not source_neg:\n",
    "                continue\n",
    "\n",
    "            ratio = (-transition_vector[source_index]) / np.sum(-transition_vector[transition_vector < 0])\n",
    "            for target_index, target_pos in zip(current_categories, pos_target_mask):\n",
    "                if target_pos:\n",
    "                    transition_count[source_index, target_index] += ratio * transition_vector[target_index]\n",
    "\n",
    "    return transition_count\n",
    "\n",
    "author_counts = {date: count_authors_by_category(author_status_metadata[date], unique_categories) for date in sorted_dates}\n",
    "author_transitions = {date: count_transitions(author_status_metadata[date], unique_categories) for date in sorted_dates}\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data to json for reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save to JSON we need to convert numpy arrays to lists and back and forth\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open('author_counts.json', 'w') as f:\n",
    "    convert_dict_of_arrays_to_dict_of_lists(author_counts)\n",
    "    json.dump(author_counts, f)\n",
    "with open('author_transitions.json', 'w') as f:\n",
    "    convert_dict_of_arrays_to_dict_of_lists(author_transitions)\n",
    "    json.dump(author_transitions, f)\n",
    "with open('unique_categories.json', 'w') as f:\n",
    "    json.dump(unique_categories, f)\n",
    "with open('sorted_dates.json', 'w') as f:\n",
    "    json.dump(sorted_dates, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way to reload the data in another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the JSON file, as a dictionary of numpy arrays\n",
    "with open('author_counts.json', 'r') as f:\n",
    "    author_counts = json.load(f)\n",
    "    convert_dict_of_lists_to_dict_of_arrays(author_counts)\n",
    "with open('author_transitions.json', 'r') as f:\n",
    "    author_transitions = json.load(f)\n",
    "    convert_dict_of_lists_to_dict_of_arrays(author_transitions)\n",
    "# Load the labels \n",
    "with open('unique_categories.json', 'r') as f:\n",
    "    unique_categories = json.load(f)\n",
    "with open('sorted_dates.json', 'r') as f:\n",
    "    sorted_dates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "16b54e8bee274342bbd0a7f5d00d3464": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48623ac8b18c450492bd15bb2c4d4374": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_51e203ba6806405a92e24f314679e30b",
        "IPY_MODEL_a328543a09c045878e5caf4381dbbd93"
       ],
       "layout": "IPY_MODEL_6cb6c8fde5a34d5384198f8c3e8b0da3"
      }
     },
     "51e203ba6806405a92e24f314679e30b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d6f5cb7b798d4235b5a6a267c4757830",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c978f7fc526e4161b9e672efc049af28",
       "value": 1
      }
     },
     "6cb6c8fde5a34d5384198f8c3e8b0da3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "898d2646573149e4b4fe65d1d497bd6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a328543a09c045878e5caf4381dbbd93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16b54e8bee274342bbd0a7f5d00d3464",
       "placeholder": "​",
       "style": "IPY_MODEL_898d2646573149e4b4fe65d1d497bd6f",
       "value": " 3360984/? [02:32&lt;00:00, 22061.79it/s]"
      }
     },
     "c978f7fc526e4161b9e672efc049af28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d6f5cb7b798d4235b5a6a267c4757830": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
